{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Norvig's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import brown, reuters, gutenberg\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import heapq\n",
        "from functools import lru_cache\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "brown_text = \" \".join(brown.words())\n",
        "reuters_text = \" \".join(reuters.words())\n",
        "gutenberg_text = \" \".join(gutenberg.words())\n",
        "all_text = brown_text + reuters_text + gutenberg_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_sentences = re.split(r'(?<=[.!?])\\s+', all_text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# ======= SPLIT DATA INTO TRAIN/TEST =======\n",
        "random.seed(42)\n",
        "random.shuffle(all_sentences)\n",
        "split_index = int(0.95 * len(all_sentences))\n",
        "train_sentences = all_sentences[:split_index]\n",
        "test_sentences = all_sentences[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_text = \" \".join(train_sentences)\n",
        "train_words = words(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "WORDS = Counter(train_words)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correction('speling')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norvigs_correction(sentence):\n",
        "    return \" \".join([correction(word) for word in sentence.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize Norvigs Solution with N-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "ngram_counts = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ngram_counts(words, max_order):\n",
        "    \"\"\"\n",
        "    Build n-gram counts for orders 1 through max_order.\n",
        "    For unigrams, keys are one-element tuples.\n",
        "    \"\"\"\n",
        "    global ngram_counts\n",
        "    for i in range(max_order):\n",
        "        if i + 1 not in ngram_counts:\n",
        "            ngram_counts[i + 1] = {}\n",
        "\n",
        "    # Order 1 (unigrams)\n",
        "    for word in words:\n",
        "        if ngram_counts[1].get((word,)) is None:\n",
        "            ngram_counts[1][(word,)] = 1\n",
        "        else:\n",
        "            ngram_counts[1][(word,)] += 1\n",
        "    \n",
        "    # Orders 2 ... max_order\n",
        "    for order in range(2, max_order + 1):\n",
        "        for i in range(len(words) - order + 1):\n",
        "            gram = tuple(words[i:i + order])\n",
        "            if ngram_counts[order].get(gram) is None:\n",
        "                ngram_counts[order][gram] = 1\n",
        "            else:\n",
        "                ngram_counts[order][gram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building n-gram counts:   0%|          | 0/231212 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building n-gram counts: 100%|██████████| 231212/231212 [00:08<00:00, 26671.51it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set the maximum n-gram order (e.g., 2 for bigrams, 3 for trigrams, etc.)\n",
        "MAX_N = 3\n",
        "for sentence in tqdm(train_sentences, desc=\"Building n-gram counts\"):\n",
        "    build_ngram_counts(words(sentence), MAX_N)\n",
        "\n",
        "total_words = len(train_words)  # Total number of words in the training set\n",
        "V = len(ngram_counts[1])  # Vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "258815"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WORDS['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "258815"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram_counts[1][('the', )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning hyperparameters for the bigram LM:\n",
            "\n",
            "Best bigram hyperparameters (alpha, ): (0.01,) with Cross Entropy: 11.374500801714818\n",
            "Tuning hyperparameters for the trigram LM:\n",
            "\n",
            "Best trigram hyperparameters (alpha, ): (0.001,) with Cross Entropy: 13.13564369458284\n",
            "Tuning hyperparameters for the interpolated LM:\n",
            "\n",
            "Best interpolated hyperparameters (alpha, lambda): (0.001, 0.5) with Cross Entropy: 11.008184862697393\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# --- Functions to compute n-gram probabilities with smoothing ---\n",
        "\n",
        "def compute_bigram_prob(w1, w2, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed bigram probability:\n",
        "      P(w2|w1) = (C(w1, w2) + alpha) / (C(w1) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w1,), 0)\n",
        "    return (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "\n",
        "def compute_trigram_prob(w1, w2, w3, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed trigram probability:\n",
        "      P(w3|w1,w2) = (C(w1, w2, w3) + alpha) / (C(w1, w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_trigram = ngram_counts.get(3, {}).get((w1, w2, w3), 0)\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    return (c_trigram + alpha) / (c_bigram + alpha * V)\n",
        "\n",
        "def compute_interpolated_prob(w1, w2, w3, alpha, lam):\n",
        "    \"\"\"\n",
        "    Computes the interpolated probability:\n",
        "      P(w3|w1,w2) = lam * P_trigram(w3|w1,w2) + (1 - lam) * P_bigram(w3|w2)\n",
        "    where the bigram probability is computed as:\n",
        "      P(w3|w2) = (C(w2, w3) + alpha) / (C(w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    # Trigram probability\n",
        "    p_tri = compute_trigram_prob(w1, w2, w3, alpha)\n",
        "    \n",
        "    # Bigram probability: note we use context w2 only\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w2, w3), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w2,), 0)\n",
        "    p_bi = (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "    \n",
        "    return lam * p_tri + (1 - lam) * p_bi\n",
        "\n",
        "# --- Functions to compute Cross Entropy and Perplexity ---\n",
        "\n",
        "def compute_cross_entropy_and_perplexity(sentences, model='interpolated', alpha=0.1, lam=0.5):\n",
        "    \"\"\"\n",
        "    Computes cross entropy and perplexity for a list of sentences.\n",
        "    \n",
        "    Parameters:\n",
        "      sentences: list of sentences (strings).\n",
        "      model: 'bigram', 'trigram', or 'interpolated'.\n",
        "      alpha: smoothing parameter.\n",
        "      lam: interpolation weight (only used if model=='interpolated').\n",
        "      \n",
        "    Returns:\n",
        "      cross_entropy, perplexity\n",
        "    \"\"\"\n",
        "    total_log_prob = 0.0\n",
        "    count = 0\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.lower().split()\n",
        "        if model == 'bigram':\n",
        "            for i in range(1, len(tokens)):\n",
        "                prob = compute_bigram_prob(tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'trigram':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_trigram_prob(tokens[i-2], tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'interpolated':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_interpolated_prob(tokens[i-2], tokens[i-1], tokens[i], alpha, lam)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "    cross_entropy = - total_log_prob / count if count > 0 else float('inf')\n",
        "    perplexity = 2 ** cross_entropy\n",
        "    return cross_entropy, perplexity\n",
        "\n",
        "# --- Hyperparameter Tuning via Grid Search ---\n",
        "\n",
        "# Use a held-out validation set (here, we take the first 1000 test sentences as an example)\n",
        "held_out = test_sentences[:10000]  # Adjust this slice based on your data\n",
        "\n",
        "# Define candidate values for alpha and lambda.\n",
        "alpha_values = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1.0]\n",
        "lam_values = [0.0001, 0.001, 0.01, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "best_bigram_params=None\n",
        "best_bigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the bigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='bigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_bigram_ce:\n",
        "        best_bigram_ce = ce\n",
        "        best_bigram_params = (alpha,)\n",
        "print(\"\\nBest bigram hyperparameters (alpha, ):\", best_bigram_params, \"with Cross Entropy:\", best_bigram_ce)\n",
        "\n",
        "best_trigram_params=None\n",
        "best_trigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the trigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='trigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_trigram_ce:\n",
        "        best_trigram_ce = ce\n",
        "        best_trigram_params = (alpha,)\n",
        "print(\"\\nBest trigram hyperparameters (alpha, ):\", best_trigram_params, \"with Cross Entropy:\", best_trigram_ce)\n",
        "\n",
        "best_interpolated_params = None\n",
        "best_interpolated_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the interpolated LM:\")\n",
        "for alpha in alpha_values:\n",
        "    for lam in lam_values:\n",
        "        ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='interpolated', alpha=alpha, lam=lam)\n",
        "        # print(f\"Alpha: {alpha}, Lambda: {lam}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "        if ce < best_interpolated_ce:\n",
        "            best_interpolated_ce = ce\n",
        "            best_interpolated_params = (alpha, lam)\n",
        "\n",
        "print(\"\\nBest interpolated hyperparameters (alpha, lambda):\", best_interpolated_params, \"with Cross Entropy:\", best_interpolated_ce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_alpha = best_interpolated_params[0]\n",
        "best_lambda = best_interpolated_params[1]\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_corrections(word):\n",
        "    # Return the list of candidate corrections.\n",
        "    # First try the word itself, then one-edit-away words.\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def my_correction(sentence, beam_width=3, n=2, alpha=best_alpha, lam=best_lambda):\n",
        "    \"\"\"\n",
        "    Corrects a sentence using an n-gram language model with tuned hyperparameters.\n",
        "    \n",
        "    Parameters:\n",
        "      - sentence: the input sentence to be corrected.\n",
        "      - beam_width: the beam width for search.\n",
        "      - n: the n-gram order to use (e.g., n=2 for bigrams, n=3 for trigrams).\n",
        "      - alpha: smoothing parameter (tuned).\n",
        "      - lam: interpolation weight for trigram and bigram probabilities (tuned).\n",
        "    \n",
        "    For each word, we use a context of up to (n-1) preceding words.\n",
        "    When there isn’t enough context (at the start), we back off to lower-order models.\n",
        "    \"\"\"\n",
        "    sentence_words = sentence.lower().split()\n",
        "    candidates_seq = [(0.0, [])]\n",
        "    \n",
        "    for i, word in enumerate(sentence_words):\n",
        "        new_candidates = []\n",
        "        candidate_list = get_corrections(word)\n",
        "        \n",
        "        for candidate in candidate_list:\n",
        "            for score, seq in candidates_seq:\n",
        "                context_length = min(n - 1, len(seq))\n",
        "                if context_length == 0:\n",
        "                    # Use unigram probability with smoothing using alpha.\n",
        "                    prob = (ngram_counts[1].get((candidate,), 0) + alpha) / (total_words + alpha * V)\n",
        "                elif context_length == 1:\n",
        "                    # Use bigram probability with smoothing.\n",
        "                    context = tuple(seq[-1:])\n",
        "                    prob = (ngram_counts.get(2, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(1, {}).get(context, 0) + alpha * V)\n",
        "                elif context_length == 2:\n",
        "                    # Use interpolated probability for trigram: \n",
        "                    # P(candidate|w1,w2) = lam * trigram + (1-lam) * bigram\n",
        "                    w1, w2 = seq[-2], seq[-1]\n",
        "                    trigram_prob = (ngram_counts.get(3, {}).get((w1, w2, candidate), 0) + alpha) / (\n",
        "                                   ngram_counts.get(2, {}).get((w1, w2), 0) + alpha * V)\n",
        "                    bigram_prob = (ngram_counts.get(2, {}).get((w2, candidate), 0) + alpha) / (\n",
        "                                  ngram_counts.get(1, {}).get((w2,), 0) + alpha * V)\n",
        "                    prob = lam * trigram_prob + (1 - lam) * bigram_prob\n",
        "                else:\n",
        "                    # For contexts longer than 2, fall back to the standard n-gram formula with smoothing.\n",
        "                    context = tuple(seq[-context_length:])\n",
        "                    prob = (ngram_counts.get(context_length + 1, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(context_length, {}).get(context, 0) + alpha * V)\n",
        "                \n",
        "                new_score = score + math.log(prob)\n",
        "                new_seq = seq + [candidate]\n",
        "                heapq.heappush(new_candidates, (new_score, new_seq))\n",
        "                if len(new_candidates) > beam_width:\n",
        "                    heapq.heappop(new_candidates)\n",
        "        \n",
        "        candidates_seq = new_candidates\n",
        "    \n",
        "    best_score, best_seq = max(candidates_seq, key=lambda x: x[0])\n",
        "    return ' '.join(best_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_noise(word, noise_prob=0.2):\n",
        "    if random.random() > noise_prob or len(word) < 1:\n",
        "        return word\n",
        "    \n",
        "    edits = []\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    \n",
        "    # Generate possible single edits\n",
        "    if len(word) > 1:\n",
        "        edits += [L + R[1:] for L, R in splits if R]  # deletes\n",
        "    if len(word) > 1:\n",
        "        edits += [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]  # transposes\n",
        "    edits += [L + c + R[1:] for L, R in splits if R for c in letters if c != R[0]]  # replaces\n",
        "    edits += [L + c + R for L, R in splits for c in letters]  # inserts\n",
        "    \n",
        "    return random.choice(edits) if edits else word\n",
        "\n",
        "# Generate test data with noise\n",
        "test_data = []\n",
        "for sentence in test_sentences:\n",
        "    original = words(sentence)\n",
        "    noisy = [add_noise(word, 0.2) for word in original]\n",
        "    test_data.append((original, noisy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvigs metrics on synhtetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "norvig_correct, total = 0, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 12170/12170 [01:22<00:00, 148.29it/s]\n"
          ]
        }
      ],
      "source": [
        "for original, noisy in tqdm(test_data, desc=\"Evaluating\"):\n",
        "    noisy_sentence = ' '.join(noisy)\n",
        "    norvig_result = norvigs_correction(noisy_sentence).split()\n",
        "    # Validate lengths\n",
        "    if len(norvig_result) != len(original):\n",
        "        continue\n",
        "\n",
        "    # Compare results\n",
        "    for o, n in zip(original, norvig_result):\n",
        "        total += 1\n",
        "        norvig_correct += (n == o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# results = []\n",
        "\n",
        "# # Hyperparameter ranges\n",
        "# n_values = range(1, 7)\n",
        "# beam_widths = range(1, 6)\n",
        "\n",
        "# # Tuning loop\n",
        "# for n in n_values:\n",
        "#     for beam_width in beam_widths:\n",
        "#         print(f\"\\nTesting n={n}, beam_width={beam_width}\")\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "        \n",
        "#         for original, noisy in tqdm(test_data, desc=f\"n={n}, bw={beam_width}\"):\n",
        "#             try:\n",
        "#                 corrected = my_correction(' '.join(noisy), \n",
        "#                                         beam_width=beam_width, \n",
        "#                                         n=n).split()\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error with n={n}, bw={beam_width}: {str(e)}\")\n",
        "#                 continue\n",
        "                \n",
        "#             if len(corrected) != len(original):\n",
        "#                 continue\n",
        "                \n",
        "#             for orig_word, corrected_word in zip(original, corrected):\n",
        "#                 total += 1\n",
        "#                 correct += (corrected_word == orig_word)\n",
        "        \n",
        "#         accuracy = correct / total if total > 0 else 0\n",
        "#         results.append({'n': n, 'beam_width': beam_width, 'accuracy': accuracy})\n",
        "#         print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# # Save results to DataFrame\n",
        "# results_df = pd.DataFrame(results)\n",
        "\n",
        "# # Save to CSV\n",
        "# results_df.to_csv('hyperparameter_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ======= LOAD AND MERGE BIGRAMS FROM FILE =======\n",
        "# bigram_file = \"useful data/bigrams (2).txt\"\n",
        "\n",
        "# with open(bigram_file, encoding=\"latin-1\") as f:\n",
        "#     for line in f:\n",
        "#         # Split by tabs (or adjust the delimiter if needed)\n",
        "#         parts = line.strip().split(\"\\t\")\n",
        "#         # Skip lines that don't have at least 3 parts (e.g., header or malformed lines)\n",
        "#         if len(parts) < 3:\n",
        "#             continue\n",
        "#         count_str, first_word, second_word = parts[:3]\n",
        "#         try:\n",
        "#             count = int(count_str)\n",
        "#         except ValueError:\n",
        "#             continue  # Skip lines where count is not an integer\n",
        "        \n",
        "#         # Create the bigram tuple\n",
        "#         bigram = (first_word, second_word)\n",
        "        \n",
        "#         # Merge: add to existing count if the bigram is already present.\n",
        "#         if bigram in ngram_counts.get(2, {}):\n",
        "#             ngram_counts[2][bigram] += count\n",
        "#         else:\n",
        "#             # Ensure that the bigram dictionary exists\n",
        "#             if 2 not in ngram_counts:\n",
        "#                 ngram_counts[2] = {}\n",
        "#             ngram_counts[2][bigram] = count\n",
        "\n",
        "\n",
        "# # Update vocabulary statistics\n",
        "# V = len(ngram_counts[1])  # Update vocabulary size after adding new words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/12170 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 12170/12170 [00:10<00:00, 1134.39it/s]\n"
          ]
        }
      ],
      "source": [
        "my_correct = 0\n",
        "\n",
        "for original, noisy in tqdm(test_data, desc=\"Evaluating\"):\n",
        "    noisy_sentence = ' '.join(noisy)\n",
        "    \n",
        "    my_result = my_correction(noisy_sentence, n=3, beam_width=5).split()\n",
        "    \n",
        "    # Validate lengths\n",
        "    if len(my_result) != len(original):\n",
        "        continue\n",
        "    \n",
        "    # Compare results\n",
        "    for o, m in zip(original, my_result):\n",
        "        my_correct += (m == o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Norvig's Accuracy: 0.9430\n",
            "Your Algorithm's Accuracy: 0.9450\n",
            "Improvement: 0.0020 (0.2%)\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics\n",
        "norvig_acc = norvig_correct / total\n",
        "my_acc = my_correct / total\n",
        "\n",
        "print(f\"\\nNorvig's Accuracy: {norvig_acc:.4f}\")\n",
        "print(f\"Your Algorithm's Accuracy: {my_acc:.4f}\")\n",
        "print(f\"Improvement: {(my_acc - norvig_acc):.4f} ({((my_acc/norvig_acc)-1)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    - List of error positions and targets\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    # Split line into text and error segments\n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        # Add preceding text\n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "        \n",
        "        if len(error.split()) > 1 or len(target.split()) > 1:\n",
        "            raw_parts.append(target)\n",
        "            continue\n",
        "        else:\n",
        "            raw_parts.append(error)\n",
        "        \n",
        "\n",
        "        if error == target:\n",
        "            continue\n",
        "\n",
        "        # Record error position\n",
        "        error_start = len(' '.join(raw_parts).split())\n",
        "        errors.append(error_start-1)\n",
        "    \n",
        "    \n",
        "    # Add remaining text\n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    # Create full sentences\n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_holbrook(test_lines, corrector_func, beam_width=None, n=None):\n",
        "    \"\"\"\n",
        "    Evaluate spelling corrector on Holbrook dataset\n",
        "    Returns:\n",
        "    - Accuracy: % of errors corrected properly\n",
        "    - Precision: % of corrections that were right\n",
        "    - Recall: % of total errors corrected\n",
        "    - F1: Harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    stats = defaultdict(int)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        # Parse line\n",
        "        raw_sentence, correct_sentence, errors = parse_holbrook_line(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        stats[\"total_errors\"] += len(errors)\n",
        "        \n",
        "        for i, (raw, corrected, correct) in enumerate(zip(raw_sentence.split(), corrected_sentence.split(), correct_sentence.split())):\n",
        "            # Count a correction if the system changed the word from raw_sentence\n",
        "            if raw != corrected:\n",
        "                stats['total_proposed'] += 1\n",
        "\n",
        "            # If the word was known to be in error and the correction is exactly right, count it as correct.\n",
        "            if i in errors and corrected == correct:\n",
        "                stats['correct'] += 1\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = stats['correct'] / stats['total_errors'] if stats['total_errors'] else 0\n",
        "    precision = stats['correct'] / stats['total_proposed'] if stats['total_proposed'] else 0\n",
        "    recall = stats['correct'] / stats['total_errors'] if stats['total_errors'] else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'total_errors': stats['total_errors'],\n",
        "        'correct_corrections': stats['correct'],\n",
        "        'incorrect_corrections': stats['incorrect']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186633\n",
            "we have got anglia like to <err targ=watch> wach </err> <err targ=cowboys> cow boys </err> .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('we have got anglia like to wach cowboys .',\n",
              " 'we have got anglia like to watch cowboys .',\n",
              " [6])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "# Load test data\n",
        "with open('holbrook-tagged.dat.txt') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print(len(text))\n",
        "\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "\n",
        "sentence_index = 17\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Norvig's solution Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:36<00:00, 28.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 17.80%\n",
            "Precision: 13.57%\n",
            "Recall: 17.80%\n",
            "F1 Score: 15.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "metrics = evaluate_holbrook(sentences, norvigs_correction)\n",
        "\n",
        "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
        "print(f\"Precision: {metrics['precision']:.2%}\")\n",
        "print(f\"Recall: {metrics['recall']:.2%}\")\n",
        "print(f\"F1 Score: {metrics['f1']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My solution metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:04<00:00, 235.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 20.12%\n",
            "Precision: 15.34%\n",
            "Recall: 20.12%\n",
            "F1 Score: 17.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "metrics = evaluate_holbrook(sentences, my_correction, n=3, beam_width=5)\n",
        "\n",
        "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
        "print(f\"Precision: {metrics['precision']:.2%}\")\n",
        "print(f\"Recall: {metrics['recall']:.2%}\")\n",
        "print(f\"F1 Score: {metrics['f1']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Word Error Rate (WER)\n",
        "\n",
        "WER is a metric used to evaluate the performance of systems that generate or correct sequences of words, such as automatic speech recognition systems or spelling correction systems. It measures the number of word-level errors made by a system when comparing its output to the reference or ground truth text.\n",
        "\n",
        "### $ WER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of substitutions\n",
        "* $ i $ : the number of insertions\n",
        "* $ D $ : the number of deletions\n",
        "* N: the total number of words in the reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line_without_indexes(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    - List of error positions and targets\n",
        "    \"\"\"\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    # Split line into text and error segments\n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        # Add preceding text\n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "\n",
        "        raw_parts.append(error)\n",
        "\n",
        "        # Record error position\n",
        "        error_start = len(' '.join(raw_parts).split())\n",
        "    \n",
        "    \n",
        "    # Add remaining text\n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    # Create full sentences\n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we have got anglia like to <err targ=watch> wach </err> <err targ=cowboys> cow boys </err> .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('we have got anglia like to wach cow boys .',\n",
              " 'we have got anglia like to watch cowboys .')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_index = 17\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line_without_indexes(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "Levenshtein =  nltk.edit_distance\n",
        "\n",
        "def evaluate_wer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "\n",
        "    total_wer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        # Parse line\n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        reference_words = correct_sentence.split()\n",
        "        corrected_words = corrected_sentence.split()\n",
        "\n",
        "        S = Levenshtein(reference_words, corrected_words)\n",
        "        I = max(0, len(corrected_words) - len(reference_words))\n",
        "        D = max(0, len(reference_words) - len(corrected_words))\n",
        "\n",
        "        N = max(len(reference_words), len(corrected_words))\n",
        "\n",
        "        wer = (S + I + D) / N\n",
        "        total_wer += wer\n",
        "\n",
        "    return total_wer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:34<00:00, 30.40it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.2265135868085441"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = evaluate_wer(sentences, norvigs_correction)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:06<00:00, 162.12it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.22391519366337606"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = evaluate_wer(sentences, my_correction, n=3, beam_width=5)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character Error Rate (CER)\n",
        "CER is similar to WER but operates at the character level. It measures the accuracy of character-level transcriptions or corrections. CER quantifies the number of character-level errors made by a system when comparing its output to the reference text. Errors include substitutions, insertions, and deletions of individual characters.\n",
        "\n",
        "### $ CER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of character substitutions\n",
        "* $ i $ : the number of character insertions\n",
        "* $ D $ : the number of character deletions\n",
        "* N: the total number of characterσ in the reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_cer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "\n",
        "    total_cer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        # Parse line\n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "\n",
        "        S = Levenshtein(correct_sentence, corrected_sentence)\n",
        "        I = max(0, len(corrected_sentence) - len(correct_sentence))\n",
        "        D = max(0, len(correct_sentence) - len(corrected_sentence))\n",
        "\n",
        "        N = max(len(correct_sentence), len(corrected_sentence))\n",
        "\n",
        "        cer = (S + I + D) / N\n",
        "        total_cer += cer\n",
        "\n",
        "    return total_cer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:36<00:00, 29.11it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.2265135868085441"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = evaluate_wer(sentences, norvigs_correction)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:01<00:00, 959.98it/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.22375511025237368"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = evaluate_wer(sentences, my_correction, n=3, beam_width=7)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

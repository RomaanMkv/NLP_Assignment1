{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview\n",
        "\n",
        "In this project, I implemented a context-sensitive spelling corrector that builds on Norvig’s spelling correction approach and enhances it with an n-gram language model. The corrector not only generates candidate corrections based on edit distance but also uses contextual information—by computing n-gram probabilities—to choose the most likely corrections within a sentence. This notebook details the steps taken, including corpus preparation, n-gram modeling, hyperparameter tuning, and finally, the context-sensitive beam search for spelling correction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "To summarize, this notebook integrates Norvig’s candidate generation method with a robust n-gram language model framework. By building and tuning bigram and trigram models (with interpolation) and using a beam search to choose the best sequence of corrections, the solution is able to perform context-sensitive spelling correction. This approach not only fixes isolated misspellings but also ensures that the corrections fit the overall sentence context, ultimately leading to more accurate and natural language output.\n",
        "\n",
        "---\n",
        "\n",
        "These markdown cells should provide a clear and detailed explanation of your implementation for your assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import brown, reuters, gutenberg\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import heapq\n",
        "from functools import lru_cache\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation and Corpus Combination\n",
        "\n",
        "I download several standard corpora available via NLTK (Brown, Reuters, and Gutenberg). The steps include:\n",
        "\n",
        "- **Combining text** from all these sources to create a rich and diverse training dataset.\n",
        "- **Splitting the data** into training and test sets (using a 95/5 split) to later evaluate the language model.\n",
        "- **Tokenizing** the training text to build a frequency-based dictionary (`WORDS`) that forms the basis for computing word probabilities.\n",
        "\n",
        "This extensive dataset helps in obtaining robust n-gram statistics for the next stage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "brown_text = \" \".join(brown.words())\n",
        "reuters_text = \" \".join(reuters.words())\n",
        "gutenberg_text = \" \".join(gutenberg.words())\n",
        "all_text = brown_text + reuters_text + gutenberg_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_sentences = re.split(r'(?<=[.!?])\\s+', all_text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "random.shuffle(all_sentences)\n",
        "split_index = int(0.95 * len(all_sentences))\n",
        "train_sentences = all_sentences[:split_index]\n",
        "test_sentences = all_sentences[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_text = \" \".join(train_sentences)\n",
        "train_words = words(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig’s Approach for Candidate Generation\n",
        "\n",
        "The first part of the code is inspired by Norvig’s solution for spelling correction. Here, I define functions to:\n",
        "\n",
        "- **Tokenize text** using a simple regex-based method.\n",
        "- **Generate candidate corrections** using the edit-distance approach (`edits1` and `edits2` functions).\n",
        "- **Filter candidates** by checking against a dictionary of known words extracted from a training corpus.\n",
        "\n",
        "This component is essential as it provides a list of potential corrections for any given misspelled word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "WORDS = Counter(train_words)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check function correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correction('speling')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norvigs_correction(sentence):\n",
        "    return \" \".join([correction(word) for word in sentence.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize Norvigs Solution with N-gram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building N-gram Counts\n",
        "\n",
        "To incorporate context, I build n-gram counts for various orders (unigrams, bigrams, trigrams):\n",
        "\n",
        "- A helper function `build_ngram_counts` is used to compute the frequency of each n-gram in the training data.\n",
        "- The counts for unigrams, bigrams, and trigrams are stored in a global dictionary (`ngram_counts`).\n",
        "\n",
        "These counts are later used to compute conditional probabilities that inform our context-sensitive corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "ngram_counts = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ngram_counts(words, max_order):\n",
        "    \"\"\"\n",
        "    Build n-gram counts for orders 1 through max_order.\n",
        "    For unigrams, keys are one-element tuples.\n",
        "    \"\"\"\n",
        "    global ngram_counts\n",
        "    for i in range(max_order):\n",
        "        if i + 1 not in ngram_counts:\n",
        "            ngram_counts[i + 1] = {}\n",
        "\n",
        "    # Order 1 (unigrams)\n",
        "    for word in words:\n",
        "        if ngram_counts[1].get((word,)) is None:\n",
        "            ngram_counts[1][(word,)] = 1\n",
        "        else:\n",
        "            ngram_counts[1][(word,)] += 1\n",
        "    \n",
        "    # Orders 2 ... max_order\n",
        "    for order in range(2, max_order + 1):\n",
        "        for i in range(len(words) - order + 1):\n",
        "            gram = tuple(words[i:i + order])\n",
        "            if ngram_counts[order].get(gram) is None:\n",
        "                ngram_counts[order][gram] = 1\n",
        "            else:\n",
        "                ngram_counts[order][gram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building n-gram counts: 100%|██████████| 231212/231212 [00:07<00:00, 30293.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set the maximum n-gram order\n",
        "MAX_N = 3\n",
        "for sentence in tqdm(train_sentences, desc=\"Building n-gram counts\"):\n",
        "    build_ngram_counts(words(sentence), MAX_N)\n",
        "\n",
        "total_words = len(train_words)  # Total number of words in the training set\n",
        "V = len(ngram_counts[1])  # Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if we correclty calculated unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert WORDS['the'] == ngram_counts[1][('the', )]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probability Smoothing and Interpolation\n",
        "\n",
        "To handle unseen n-grams and avoid zero probabilities, I implemented smoothing techniques:\n",
        "\n",
        "- **Bigram and Trigram Smoothing:** For each, I compute smoothed probabilities using Laplace (add-alpha) smoothing.\n",
        "- **Interpolated Probabilities:** For contexts where more than one n-gram level is available, I combine trigram and bigram probabilities using a weighted interpolation scheme. The interpolation weight (`lam`) determines how much influence the trigram vs. bigram model has on the final probability.\n",
        "\n",
        "These probability functions allow us to estimate the likelihood of a word given its context in the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Language Model\n",
        "\n",
        "To assess the quality of our n-gram language models, I implemented functions to compute:\n",
        "\n",
        "- **Cross Entropy:** Measures the average number of bits needed to encode a sentence.\n",
        "- **Perplexity:** A transformed version of cross entropy that provides a more intuitive measure of model performance.\n",
        "\n",
        "This evaluation is performed on a held-out set (a subset of the test sentences), providing a way to compare different models (bigram, trigram, and interpolated) and select the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bigram_prob(w1, w2, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed bigram probability:\n",
        "      P(w2|w1) = (C(w1, w2) + alpha) / (C(w1) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w1,), 0)\n",
        "    return (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "\n",
        "def compute_trigram_prob(w1, w2, w3, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed trigram probability:\n",
        "      P(w3|w1,w2) = (C(w1, w2, w3) + alpha) / (C(w1, w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_trigram = ngram_counts.get(3, {}).get((w1, w2, w3), 0)\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    return (c_trigram + alpha) / (c_bigram + alpha * V)\n",
        "\n",
        "def compute_interpolated_prob(w1, w2, w3, alpha, lam):\n",
        "    \"\"\"\n",
        "    Computes the interpolated probability:\n",
        "      P(w3|w1,w2) = lam * P_trigram(w3|w1,w2) + (1 - lam) * P_bigram(w3|w2)\n",
        "    where the bigram probability is computed as:\n",
        "      P(w3|w2) = (C(w2, w3) + alpha) / (C(w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    # Trigram probability\n",
        "    p_tri = compute_trigram_prob(w1, w2, w3, alpha)\n",
        "    \n",
        "    # Bigram probability: note we use context w2 only\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w2, w3), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w2,), 0)\n",
        "    p_bi = (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "    \n",
        "    return lam * p_tri + (1 - lam) * p_bi\n",
        "\n",
        "\n",
        "def compute_cross_entropy_and_perplexity(sentences, model='interpolated', alpha=0.1, lam=0.5):\n",
        "    \"\"\"\n",
        "    Computes cross entropy and perplexity for a list of sentences.\n",
        "    \n",
        "    Parameters:\n",
        "      sentences: list of sentences (strings).\n",
        "      model: 'bigram', 'trigram', or 'interpolated'.\n",
        "      alpha: smoothing parameter.\n",
        "      lam: interpolation weight (only used if model=='interpolated').\n",
        "      \n",
        "    Returns:\n",
        "      cross_entropy, perplexity\n",
        "    \"\"\"\n",
        "    total_log_prob = 0.0\n",
        "    count = 0\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.lower().split()\n",
        "        if model == 'bigram':\n",
        "            for i in range(1, len(tokens)):\n",
        "                prob = compute_bigram_prob(tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'trigram':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_trigram_prob(tokens[i-2], tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'interpolated':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_interpolated_prob(tokens[i-2], tokens[i-1], tokens[i], alpha, lam)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "    cross_entropy = - total_log_prob / count if count > 0 else float('inf')\n",
        "    perplexity = 2 ** cross_entropy\n",
        "    return cross_entropy, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning via Grid Search\n",
        "\n",
        "I performed a grid search over candidate values for the smoothing parameter (`alpha`) and the interpolation weight (`lam`):\n",
        "\n",
        "- For the **bigram** and **trigram** models, I tuned the smoothing parameter.\n",
        "- For the **interpolated** model, both `alpha` and `lam` were tuned.\n",
        "- The best parameters were chosen based on the minimum cross entropy achieved on the held-out validation set.\n",
        "\n",
        "This tuning ensures that the language model is well-calibrated and provides reliable probability estimates for the subsequent correction task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning hyperparameters for the bigram LM:\n",
            "Best bigram hyperparameters (alpha, ): (0.01,) with Cross Entropy: 11.374500801714818 \n",
            "\n",
            "Tuning hyperparameters for the trigram LM:\n",
            "Best trigram hyperparameters (alpha, ): (0.001,) with Cross Entropy: 13.13564369458284 \n",
            "\n",
            "Tuning hyperparameters for the interpolated LM:\n",
            "Best interpolated hyperparameters (alpha, lambda): (0.001, 0.5) with Cross Entropy: 11.008184862697393 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "held_out = test_sentences[:10000]\n",
        "\n",
        "# Define candidate values for alpha and lambda.\n",
        "alpha_values = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1.0]\n",
        "lam_values = [0.0001, 0.001, 0.01, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "best_bigram_params=None\n",
        "best_bigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the bigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='bigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_bigram_ce:\n",
        "        best_bigram_ce = ce\n",
        "        best_bigram_params = (alpha,)\n",
        "print(\"Best bigram hyperparameters (alpha, ):\", best_bigram_params, \"with Cross Entropy:\", best_bigram_ce, \"\\n\")\n",
        "\n",
        "best_trigram_params=None\n",
        "best_trigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the trigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='trigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_trigram_ce:\n",
        "        best_trigram_ce = ce\n",
        "        best_trigram_params = (alpha,)\n",
        "print(\"Best trigram hyperparameters (alpha, ):\", best_trigram_params, \"with Cross Entropy:\", best_trigram_ce, \"\\n\")\n",
        "\n",
        "best_interpolated_params = None\n",
        "best_interpolated_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the interpolated LM:\")\n",
        "for alpha in alpha_values:\n",
        "    for lam in lam_values:\n",
        "        ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='interpolated', alpha=alpha, lam=lam)\n",
        "        # print(f\"Alpha: {alpha}, Lambda: {lam}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "        if ce < best_interpolated_ce:\n",
        "            best_interpolated_ce = ce\n",
        "            best_interpolated_params = (alpha, lam)\n",
        "\n",
        "print(\"Best interpolated hyperparameters (alpha, lambda):\", best_interpolated_params, \"with Cross Entropy:\", best_interpolated_ce, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The best model is interpolated**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context-Sensitive Correction using Beam Search\n",
        "\n",
        "The final component of the solution is a context-sensitive correction function (`my_correction`):\n",
        "\n",
        "- **Beam Search:** Instead of correcting each word independently, the beam search maintains a set of the best candidate sequences (with a defined beam width) as it processes the sentence word by word.\n",
        "- **Context-aware Scoring:** For each word, candidate corrections are scored based on the n-gram probability computed with the appropriate context. Early in the sentence (with little context), unigram or bigram probabilities are used; with more context, interpolated trigram probabilities are applied.\n",
        "- **Caching Corrections:** The `lru_cache` decorator is used to speed up candidate generation by caching results for repeated words.\n",
        "\n",
        "This approach enables the corrector to select a globally coherent sequence of words, thus effectively addressing context-sensitive ambiguities (e.g., distinguishing “doing sport” from “dying sport” based on the surrounding words).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_alpha = best_interpolated_params[0]\n",
        "best_lambda = best_interpolated_params[1]\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_corrections(word):\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def my_correction(sentence, beam_width=3, n=2, alpha=best_alpha, lam=best_lambda):\n",
        "    \"\"\"\n",
        "    Corrects a sentence using an n-gram language model with tuned hyperparameters.\n",
        "    \n",
        "    Parameters:\n",
        "      - sentence: the input sentence to be corrected.\n",
        "      - beam_width: the beam width for search.\n",
        "      - n: the n-gram order to use (e.g., n=2 for bigrams, n=3 for trigrams).\n",
        "      - alpha: smoothing parameter (tuned).\n",
        "      - lam: interpolation weight for trigram and bigram probabilities (tuned).\n",
        "    \n",
        "    For each word, we use a context of up to (n-1) preceding words.\n",
        "    When there isn’t enough context (at the start), we back off to lower-order models.\n",
        "    \"\"\"\n",
        "    sentence_words = sentence.lower().split()\n",
        "    candidates_seq = [(0.0, [])]\n",
        "    \n",
        "    for i, word in enumerate(sentence_words):\n",
        "        new_candidates = []\n",
        "        candidate_list = get_corrections(word)\n",
        "        \n",
        "        for candidate in candidate_list:\n",
        "            for score, seq in candidates_seq:\n",
        "                context_length = min(n - 1, len(seq))\n",
        "                if context_length == 0:\n",
        "                    # Use unigram probability with smoothing using alpha.\n",
        "                    prob = (ngram_counts[1].get((candidate,), 0) + alpha) / (total_words + alpha * V)\n",
        "                elif context_length == 1:\n",
        "                    # Use bigram probability with smoothing.\n",
        "                    context = tuple(seq[-1:])\n",
        "                    prob = (ngram_counts.get(2, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(1, {}).get(context, 0) + alpha * V)\n",
        "                elif context_length == 2:\n",
        "                    # Use interpolated probability for trigram: \n",
        "                    # P(candidate|w1,w2) = lam * trigram + (1-lam) * bigram\n",
        "                    w1, w2 = seq[-2], seq[-1]\n",
        "                    trigram_prob = (ngram_counts.get(3, {}).get((w1, w2, candidate), 0) + alpha) / (\n",
        "                                   ngram_counts.get(2, {}).get((w1, w2), 0) + alpha * V)\n",
        "                    bigram_prob = (ngram_counts.get(2, {}).get((w2, candidate), 0) + alpha) / (\n",
        "                                  ngram_counts.get(1, {}).get((w2,), 0) + alpha * V)\n",
        "                    prob = lam * trigram_prob + (1 - lam) * bigram_prob\n",
        "                else:\n",
        "                    # For contexts longer than 2, fall back to the standard n-gram formula with smoothing.\n",
        "                    context = tuple(seq[-context_length:])\n",
        "                    prob = (ngram_counts.get(context_length + 1, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(context_length, {}).get(context, 0) + alpha * V)\n",
        "                \n",
        "                new_score = score + math.log(prob)\n",
        "                new_seq = seq + [candidate]\n",
        "                heapq.heappush(new_candidates, (new_score, new_seq))\n",
        "                if len(new_candidates) > beam_width:\n",
        "                    heapq.heappop(new_candidates)\n",
        "        \n",
        "        candidates_seq = new_candidates\n",
        "    \n",
        "    best_score, best_seq = max(candidates_seq, key=lambda x: x[0])\n",
        "    return ' '.join(best_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid Search for Optimal Beam Width\n",
        "\n",
        "In this section, we perform a grid search to determine the best `beam_width` for our context-sensitive spelling correction algorithm. The `beam_width` parameter controls how many candidate correction sequences are maintained during the beam search. While a higher beam width might capture more promising candidate sequences and potentially yield higher accuracy, it also increases computational cost.\n",
        "\n",
        "For this grid search:\n",
        "- We evaluate a range of beam widths (from 1 to 10).\n",
        "- For each beam width, we add noise to sentences (using our `add_noise` function) to simulate spelling errors.\n",
        "- We run the `my_correction` function on the noisy sentences (with `n=3` for trigram context) and compare the output with the original sentence.\n",
        "- The overall accuracy is calculated as the proportion of words correctly recovered.\n",
        "- Finally, we plot the beam width values against their corresponding accuracies to visualize and select the best parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train data: 100%|██████████| 100/100 [00:11<00:00,  8.74it/s]\n"
          ]
        }
      ],
      "source": [
        "def add_noise(word, noise_prob=0.2):\n",
        "    if random.random() > noise_prob or len(word) < 1:\n",
        "        return word\n",
        "    edits_set = set()\n",
        "\n",
        "    for edit1_word in edits1(word):\n",
        "        for edit2_word in edits1(edit1_word):\n",
        "            edits_set.add(edit2_word)\n",
        "    \n",
        "    edits_set = edits_set.union(edits1(word))\n",
        "    edits = list(edits_set)\n",
        "        \n",
        "    return random.choice(edits) if edits else word\n",
        "\n",
        "test_data = []\n",
        "for sentence in tqdm(train_sentences[:100], desc=\"Generating synthetic train data\"):\n",
        "    original = words(sentence)\n",
        "    noisy = [add_noise(word, 0.2) for word in original]\n",
        "    test_data.append([original, noisy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting grid search for beam width...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=1: 100%|██████████| 100/100 [00:13<00:00,  7.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 1: Accuracy = 0.9126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=2: 100%|██████████| 100/100 [00:00<00:00, 5238.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 2: Accuracy = 0.9158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=3: 100%|██████████| 100/100 [00:00<00:00, 3876.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 3: Accuracy = 0.9163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=4: 100%|██████████| 100/100 [00:00<00:00, 3092.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 4: Accuracy = 0.9172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=5: 100%|██████████| 100/100 [00:00<00:00, 2693.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 5: Accuracy = 0.9177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=6: 100%|██████████| 100/100 [00:00<00:00, 2126.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 6: Accuracy = 0.9172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=7: 100%|██████████| 100/100 [00:00<00:00, 1599.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 7: Accuracy = 0.9181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=8: 100%|██████████| 100/100 [00:00<00:00, 1802.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 8: Accuracy = 0.9181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=9: 100%|██████████| 100/100 [00:00<00:00, 1702.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 9: Accuracy = 0.9186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=10: 100%|██████████| 100/100 [00:00<00:00, 1518.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 10: Accuracy = 0.9181\n",
            "\n",
            "Best beam_width: 9 with Accuracy: 0.9186\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "beam_width_values = list(range(1, 11))\n",
        "accuracy_results = []\n",
        "\n",
        "print(\"Starting grid search for beam width...\\n\")\n",
        "\n",
        "for bw in beam_width_values:\n",
        "    my_correct, total = 0, 0\n",
        "    for original, noisy in tqdm(test_data, desc=f\"Evaluating beam_width={bw}\"):\n",
        "        noisy_sentence = ' '.join(noisy)\n",
        "        my_result = my_correction(noisy_sentence, n=3, beam_width=bw).split()\n",
        "        \n",
        "        if len(my_result) != len(original):\n",
        "            continue\n",
        "        \n",
        "        for o, m in zip(original, my_result):\n",
        "            total += 1\n",
        "            if m == o:\n",
        "                my_correct += 1\n",
        "    \n",
        "    acc = my_correct / total if total > 0 else 0\n",
        "    accuracy_results.append(acc)\n",
        "    print(f\"Beam width {bw}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "best_idx = accuracy_results.index(max(accuracy_results))\n",
        "best_beam_width = beam_width_values[best_idx]\n",
        "print(f\"\\nBest beam_width: {best_beam_width} with Accuracy: {accuracy_results[best_idx]:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(beam_width_values, accuracy_results, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Beam Width\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Beam Width vs. Spelling Correction Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Test Data with Noise\n",
        "\n",
        "In this section, we create synthetic test data by adding noise to the original test sentences. For each sentence in our test set, we:\n",
        "- Tokenize the sentence into words.\n",
        "- Introduce noise to each word using the `add_noise` function with a noise probability of 0.3. This simulates typical misspellings.\n",
        "- Store both the original and the noisy version of the sentence in a list (`test_data`) for later evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = []\n",
        "for sentence in tqdm(test_sentences, desc=\"Generating synthetic test data\"):\n",
        "    original = words(sentence)\n",
        "    noisy = [add_noise(word, 0.3) for word in original]\n",
        "    test_data.append([original, noisy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Norvig's Correction on Synthetic Data\n",
        "\n",
        "Here, we evaluate the performance of the baseline Norvig's correction algorithm on the synthetic (noisy) test data:\n",
        "- For each noisy sentence, we generate a corrected sentence using `norvigs_correction`.\n",
        "- We ensure that the corrected sentence has the same number of words as the original.\n",
        "- We compare the corrected words with the original words on a word-by-word basis.\n",
        "- Finally, we compute and print Norvig’s accuracy, which is the ratio of correctly recovered words to the total words evaluated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 100/100 [00:14<00:00,  6.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Norvig's Accuracy: 0.9079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "norvig_correct, total = 0, 0\n",
        "\n",
        "for original, noisy in tqdm(test_data, desc=\"Evaluating\"):\n",
        "    noisy_sentence = ' '.join(noisy)\n",
        "    norvig_result = norvigs_correction(noisy_sentence).split()\n",
        "\n",
        "    if len(norvig_result) != len(original):\n",
        "        continue\n",
        "\n",
        "    for o, n in zip(original, norvig_result):\n",
        "        total += 1\n",
        "        norvig_correct += (n == o)\n",
        "\n",
        "norvig_acc = norvig_correct / total\n",
        "print(f\"\\nNorvig's Accuracy: {norvig_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the NGram Algorithm with Beam Search\n",
        "\n",
        "Next, we evaluate our enhanced context-sensitive spelling corrector that uses an n-gram language model combined with beam search:\n",
        "- The function `my_correction` is applied to each noisy sentence with the beam width set to the best value obtained from previous experiments and `n=3` for trigram context.\n",
        "- Similar to the Norvig baseline, we validate that the corrected sentence length matches the original.\n",
        "- We compare each word of the corrected sentence with the corresponding original word.\n",
        "- The accuracy of the NGram-based algorithm is computed as the fraction of words correctly recovered.\n",
        "- The result is printed for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 100/100 [00:00<00:00, 1533.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGram algorithm Accuracy: 0.9186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "my_correct, total = 0, 0\n",
        "\n",
        "for original, noisy in tqdm(test_data, desc=\"Evaluating\"):\n",
        "    noisy_sentence = ' '.join(noisy)\n",
        "    \n",
        "    my_result = my_correction(noisy_sentence, n=3, beam_width=best_beam_width).split()\n",
        "    \n",
        "    if len(my_result) != len(original):\n",
        "        continue\n",
        "    \n",
        "    for o, m in zip(original, my_result):\n",
        "        total += 1\n",
        "        my_correct += (m == o)\n",
        "\n",
        "my_acc = my_correct / total\n",
        "print(f\"NGram algorithm Accuracy: {my_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing the Two Approaches and Reporting Improvement\n",
        "\n",
        "In this final step:\n",
        "- We repeat the evaluation for both Norvig's and our NGram algorithm on the same synthetic test data.\n",
        "- After obtaining the accuracies from both methods, we print the improvement in performance.\n",
        "- The improvement is expressed both as an absolute difference in accuracy and as a percentage increase relative to Norvig's baseline.\n",
        "  \n",
        "This comparison highlights the benefits of incorporating context-sensitive n-gram information and beam search over the simpler correction approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improvement: 0.0107 (1.2%)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Improvement: {(my_acc - norvig_acc):.4f} ({((my_acc/norvig_acc)-1)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on a Test Set\n",
        "\n",
        "In this section, we evaluate our spelling correction system using a real-world dataset – the Holbrook corpus. The Holbrook dataset consists of passages taken from the book *English for the Rejected* by David Holbrook. These passages are unique because they include genuine writing errors from secondary-school children. The original misspellings are tagged, which allows us to compare the corrected output against the intended words.\n",
        "\n",
        "We will compare the performance of two systems:\n",
        "- **Norvig's Corrector:** Our baseline implementation based on Norvig's algorithm.\n",
        "- **NGram-based Corrector:** Our enhanced context-sensitive corrector that uses an n-gram language model with beam search.\n",
        "\n",
        "The evaluation metrics include accuracy, precision, recall, and F1 score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing the Holbrook Dataset\n",
        "\n",
        "The function `parse_holbrook_line` processes each line from the Holbrook dataset. It extracts:\n",
        "- The **raw sentence** (with errors),\n",
        "- The **correct sentence** (with target words), and\n",
        "- The **error positions** (indexes where errors occurred).\n",
        "\n",
        "This parsing is essential for aligning the outputs of our correctors with the reference text, ensuring we know exactly where corrections should have been made.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    - List of error positions and targets\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    # Split line into text and error segments\n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "        \n",
        "        if len(error.split()) > 1 or len(target.split()) > 1:\n",
        "            raw_parts.append(target)\n",
        "            continue\n",
        "        else:\n",
        "            raw_parts.append(error)\n",
        "        \n",
        "\n",
        "        if error == target:\n",
        "            continue\n",
        "\n",
        "        error_start = len(' '.join(raw_parts).split())\n",
        "        errors.append(error_start-1)\n",
        "    \n",
        "    \n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence, errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Metrics on the Holbrook Dataset\n",
        "\n",
        "The `evaluate_holbrook` function evaluates a given spelling corrector on the parsed Holbrook data. For each sentence, it:\n",
        "- Applies the correction function (either Norvig's or our NGram-based method).\n",
        "- Compares the corrected sentence to the correct sentence (the ground truth).\n",
        "- Counts the total number of errors, the number of corrections proposed by the system, and the number of correct corrections.\n",
        "\n",
        "From these counts, we calculate:\n",
        "- **Accuracy:** The percentage of errors that were correctly corrected.\n",
        "- **Precision:** The percentage of proposed corrections that were actually correct.\n",
        "- **Recall:** The percentage of total errors that the system corrected.\n",
        "- **F1 Score:** The harmonic mean of precision and recall.\n",
        "\n",
        "These metrics provide a comprehensive view of each system's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_holbrook(test_lines, corrector_func, beam_width=None, n=None):\n",
        "    \"\"\"\n",
        "    Evaluate spelling corrector on Holbrook dataset\n",
        "    Returns:\n",
        "    - Accuracy: % of errors corrected properly\n",
        "    - Precision: % of corrections that were right\n",
        "    - Recall: % of total errors corrected\n",
        "    - F1: Harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    stats = defaultdict(int)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence, errors = parse_holbrook_line(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        stats[\"total_errors\"] += len(errors)\n",
        "        \n",
        "        for i, (raw, corrected, correct) in enumerate(zip(raw_sentence.split(), corrected_sentence.split(), correct_sentence.split())):\n",
        "            if raw != corrected:\n",
        "                stats['total_proposed'] += 1\n",
        "\n",
        "            if i in errors and corrected == correct:\n",
        "                stats['correct'] += 1\n",
        "    \n",
        "    accuracy = stats['correct'] / stats['total_errors'] if stats['total_errors'] else 0\n",
        "    precision = stats['correct'] / stats['total_proposed'] if stats['total_proposed'] else 0\n",
        "    recall = stats['correct'] / stats['total_errors'] if stats['total_errors'] else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'total_errors': stats['total_errors'],\n",
        "        'correct_corrections': stats['correct'],\n",
        "        'incorrect_corrections': stats['incorrect']\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading and Inspecting the Holbrook Data\n",
        "\n",
        "We load the Holbrook dataset from the file `holbrook-tagged.dat.txt` and split the text into sentences using regular expressions. A sample sentence is printed and parsed to illustrate how the errors are tagged and how the parsing function works. This step ensures that our evaluation functions receive correctly formatted data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186633\n",
            "we have got anglia like to <err targ=watch> wach </err> <err targ=cowboys> cow boys </err> .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('we have got anglia like to wach cowboys .',\n",
              " 'we have got anglia like to watch cowboys .',\n",
              " [6])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('holbrook-tagged.dat.txt') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print(len(text))\n",
        "\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "\n",
        "sentence_index = 17\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Norvig's Correction on the Holbrook Dataset\n",
        "\n",
        "The first evaluation is performed using Norvig's correction function. After processing the entire dataset with `evaluate_holbrook`, we print out the computed accuracy, precision, recall, and F1 score. This serves as the baseline performance for our spelling correction system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:30<00:00, 34.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 17.84%\n",
            "Precision: 13.60%\n",
            "Recall: 17.84%\n",
            "F1 Score: 15.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "norvig_metrics = evaluate_holbrook(sentences, norvigs_correction)\n",
        "\n",
        "print(f\"Accuracy: {norvig_metrics['accuracy']:.2%}\")\n",
        "print(f\"Precision: {norvig_metrics['precision']:.2%}\")\n",
        "print(f\"Recall: {norvig_metrics['recall']:.2%}\")\n",
        "print(f\"F1 Score: {norvig_metrics['f1']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the NGram-based Correction on the Holbrook Dataset\n",
        "\n",
        "Next, we evaluate our enhanced correction system (which uses beam search with a trigram language model) on the same dataset. In this case, we call `evaluate_holbrook` with the additional parameters `n=3` (for trigram context) and an appropriate `beam_width` (e.g., 5). The results are printed and later compared with those from Norvig's corrector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:00<00:00, 2304.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 21.04%\n",
            "Precision: 16.04%\n",
            "Recall: 21.04%\n",
            "F1 Score: 18.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ngram_metrics = evaluate_holbrook(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "\n",
        "print(f\"Accuracy: {ngram_metrics['accuracy']:.2%}\")\n",
        "print(f\"Precision: {ngram_metrics['precision']:.2%}\")\n",
        "print(f\"Recall: {ngram_metrics['recall']:.2%}\")\n",
        "print(f\"F1 Score: {ngram_metrics['f1']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing the Results\n",
        "\n",
        "After obtaining the metrics for both approaches, we calculate and print the differences between the algorithms' results. This comparison includes:\n",
        "- The improvement in accuracy,\n",
        "- The improvement in precision, recall, and F1 score,\n",
        "- And any other observed benefits in error reduction.\n",
        "\n",
        "This side-by-side comparison helps illustrate the impact of incorporating context through the n-gram model and beam search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparative Improvement:\n",
            "Accuracy Improvement: 3.20%\n",
            "Precision Improvement: 2.44%\n",
            "Recall Improvement: 3.20%\n",
            "F1 Score Improvement: 2.76%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nComparative Improvement:\")\n",
        "accuracy_improvement = ngram_metrics['accuracy'] - norvig_metrics['accuracy']\n",
        "precision_improvement = ngram_metrics['precision'] - norvig_metrics['precision']\n",
        "recall_improvement = ngram_metrics['recall'] - norvig_metrics['recall']\n",
        "f1_improvement = ngram_metrics['f1'] - norvig_metrics['f1']\n",
        "\n",
        "print(f\"Accuracy Improvement: {accuracy_improvement:.2%}\")\n",
        "print(f\"Precision Improvement: {precision_improvement:.2%}\")\n",
        "print(f\"Recall Improvement: {recall_improvement:.2%}\")\n",
        "print(f\"F1 Score Improvement: {f1_improvement:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Word Error Rate (WER)\n",
        "\n",
        "WER is a widely used metric in tasks like speech recognition and spelling correction. It is defined as:\n",
        "\n",
        "### $ WER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of substitutions\n",
        "* $ i $ : the number of insertions\n",
        "* $ D $ : the number of deletions\n",
        "* N: the total number of words in the reference\n",
        "\n",
        "To compute WER, we first define a helper function `parse_holbrook_line_without_indexes` which returns the raw and correct sentences without the error index information. The `evaluate_wer` function then uses the `nltk.edit_distance` function (i.e., Levenshtein distance) on word sequences to determine the number of word-level errors. We run this evaluation for both Norvig's corrector and our NGram-based corrector to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line_without_indexes(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    \"\"\"\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "\n",
        "        raw_parts.append(error)\n",
        "\n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my <err targ=sister> siter </err> <err targ=goes> go </err> to tonbury.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('my siter go to tonbury.', 'my sister goes to tonbury.')"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_index = 3\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line_without_indexes(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Levenshtein =  nltk.edit_distance\n",
        "\n",
        "def evaluate_wer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "\n",
        "    total_wer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        reference_words = correct_sentence.split()\n",
        "        corrected_words = corrected_sentence.split()\n",
        "\n",
        "        S = Levenshtein(reference_words, corrected_words)\n",
        "        I = max(0, len(corrected_words) - len(reference_words))\n",
        "        D = max(0, len(reference_words) - len(corrected_words))\n",
        "\n",
        "        N = max(len(reference_words), len(corrected_words))\n",
        "\n",
        "        wer = (S + I + D) / N\n",
        "        total_wer += wer\n",
        "\n",
        "    return total_wer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:31<00:00, 33.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's WER: 0.22640846749812676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "wer_norvig = evaluate_wer(sentences, norvigs_correction)\n",
        "print(\"Norvig's WER:\", wer_norvig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:01<00:00, 984.32it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGram-based WER: 0.22354008388812138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "wer_ngram = evaluate_wer(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "print(\"NGram-based WER:\", wer_ngram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER Improvement (Norvig's WER - NGram-based WER): 0.0028683836100053817\n"
          ]
        }
      ],
      "source": [
        "improvement_wer = wer_norvig - wer_ngram\n",
        "print(\"WER Improvement (Norvig's WER - NGram-based WER):\", improvement_wer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character Error Rate (CER)\n",
        "\n",
        "CER is similar to WER but is calculated at the character level. This metric is particularly useful for capturing fine-grained errors that might be overlooked by word-level evaluation. The formula for CER is:\n",
        "\n",
        "\n",
        "### $ CER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of character substitutions\n",
        "* $ i $ : the number of character insertions\n",
        "* $ D $ : the number of character deletions\n",
        "* N: the total number of characterσ in the reference\n",
        "\n",
        "where the variables represent the number of character substitutions, insertions, deletions, and the total number of characters in the reference, respectively.\n",
        "\n",
        "The `evaluate_cer` function computes CER by comparing the corrected output and the reference at the character level using the Levenshtein distance. We evaluate both Norvig's and our NGram-based corrector on the Holbrook dataset and compare the resulting CER scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_cer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "    total_cer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        S = Levenshtein(correct_sentence, corrected_sentence)\n",
        "        I = max(0, len(corrected_sentence) - len(correct_sentence))\n",
        "        D = max(0, len(correct_sentence) - len(corrected_sentence))\n",
        "\n",
        "        N = max(len(correct_sentence), len(corrected_sentence))\n",
        "\n",
        "        cer = (S + I + D) / N\n",
        "        total_cer += cer\n",
        "\n",
        "    return total_cer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/1057 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:47<00:00, 22.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's WER: 0.10758515872293266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cer_norvig = evaluate_cer(sentences, norvigs_correction)\n",
        "print(\"Norvig's WER:\", cer_norvig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:15<00:00, 70.38it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGram-based WER: 0.10491248971293692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cer_ngram = evaluate_cer(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "print(\"NGram-based WER:\", cer_ngram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER Improvement (Norvig's WER - NGram-based WER): 0.002672669009995743\n"
          ]
        }
      ],
      "source": [
        "improvement_cer = cer_norvig - cer_ngram\n",
        "print(\"WER Improvement (Norvig's WER - NGram-based WER):\", improvement_cer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reporting Differences Between Algorithms\n",
        "\n",
        "Finally, we calculate the differences between the performance metrics (accuracy, WER, CER, etc.) of Norvig's corrector and our NGram-based corrector. These differences are printed out, providing a clear picture of how much our improvements have enhanced the overall spelling correction performance. This comparative analysis is crucial for validating the effectiveness of the context-sensitive approach.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview\n",
        "\n",
        "In this project, I implemented a context-sensitive spelling corrector that builds on Norvig’s spelling correction approach and enhances it with an n-gram language model. The corrector not only generates candidate corrections based on edit distance but also uses contextual information—by computing n-gram probabilities—to choose the most likely corrections within a sentence. This notebook details the steps taken, including corpus preparation, n-gram modeling, hyperparameter tuning, and finally, the context-sensitive beam search for spelling correction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import brown, reuters, gutenberg\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import heapq\n",
        "from functools import lru_cache\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation and Corpus Combination\n",
        "\n",
        "I download several standard corpora available via NLTK (Brown, Reuters, and Gutenberg). The steps include:\n",
        "\n",
        "- **Combining text** from all these sources to create a rich and diverse training dataset.\n",
        "- **Tokenizing** the training text to build a frequency-based dictionary (`WORDS`) that forms the basis for computing word probabilities.\n",
        "\n",
        "This extensive dataset helps in obtaining robust n-gram statistics for the next stage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "brown_text = \" \".join(brown.words())\n",
        "reuters_text = \" \".join(reuters.words())\n",
        "gutenberg_text = \" \".join(gutenberg.words())\n",
        "all_text = brown_text + reuters_text + gutenberg_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sentences = re.split(r'(?<=[.!?])\\s+', all_text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_text = \" \".join(train_sentences)\n",
        "train_words = words(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig’s Approach for Candidate Generation\n",
        "\n",
        "The first part of the code is inspired by Norvig’s solution for spelling correction. Here, I define functions to:\n",
        "\n",
        "- **Tokenize text** using a simple regex-based method.\n",
        "- **Generate candidate corrections** using the edit-distance approach (`edits1` and `edits2` functions).\n",
        "- **Filter candidates** by checking against a dictionary of known words extracted from a training corpus.\n",
        "\n",
        "This component is essential as it provides a list of potential corrections for any given misspelled word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "WORDS = Counter(train_words)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check function correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'spelling'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "correction('speling')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norvigs_correction(sentence):\n",
        "    return \" \".join([correction(word) for word in sentence.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize Norvigs Solution with N-gram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building N-gram Counts\n",
        "\n",
        "To incorporate context, I build n-gram counts for various orders (unigrams, bigrams, trigrams):\n",
        "\n",
        "- A helper function `build_ngram_counts` is used to compute the frequency of each n-gram in the training data.\n",
        "- The counts for unigrams, bigrams, and trigrams are stored in a global dictionary (`ngram_counts`).\n",
        "\n",
        "These counts are later used to compute conditional probabilities that inform our context-sensitive corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ngram_counts = {}\n",
        "text = all_text.lower()\n",
        "text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "tokens = re.findall(r'\\w+', text)\n",
        "tokens = [token for token in tokens if len(token) > 1]\n",
        "word_counts = Counter(tokens)\n",
        "my_vocab = {word for word, count in word_counts.items() if count >= 8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ngram_counts(words_list, max_order, vocab):\n",
        "    global ngram_counts\n",
        "    for i in range(max_order):\n",
        "        if i + 1 not in ngram_counts:\n",
        "            ngram_counts[i + 1] = {}\n",
        "\n",
        "    # Order 1 (unigrams)\n",
        "    for word in words_list:\n",
        "        if word in vocab:\n",
        "            ngram_counts[1][(word,)] = ngram_counts[1].get((word,), 0) + 1\n",
        "    \n",
        "    # Orders 2 ... max_order\n",
        "    for order in range(2, max_order + 1):\n",
        "        for i in range(len(words_list) - order + 1):\n",
        "            gram = tuple(words_list[i:i + order])\n",
        "            # Only include the n-gram if all words are in the filtered vocab.\n",
        "            if all(w in vocab for w in gram):\n",
        "                ngram_counts[order][gram] = ngram_counts[order].get(gram, 0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building n-gram counts: 100%|██████████| 243382/243382 [00:10<00:00, 23236.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set the maximum n-gram order\n",
        "MAX_N = 3\n",
        "for sentence in tqdm(train_sentences, desc=\"Building n-gram counts\"):\n",
        "    build_ngram_counts(words(sentence), MAX_N, my_vocab)\n",
        "\n",
        "V = len(ngram_counts[1])  # Vocabulary size\n",
        "total_words = V  # Total number of words in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if we correclty calculated unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert WORDS['the'] == ngram_counts[1][('the', )]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probability Smoothing and Interpolation\n",
        "\n",
        "To handle unseen n-grams and avoid zero probabilities, I implemented smoothing techniques:\n",
        "\n",
        "- **Bigram and Trigram Smoothing:** For each, I compute smoothed probabilities using Laplace (add-alpha) smoothing.\n",
        "- **Interpolated Probabilities:** For contexts where more than one n-gram level is available, I combine trigram and bigram probabilities using a weighted interpolation scheme. The interpolation weight (`lam`) determines how much influence the trigram vs. bigram model has on the final probability.\n",
        "\n",
        "These probability functions allow us to estimate the likelihood of a word given its context in the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Language Model\n",
        "\n",
        "To assess the quality of our n-gram language models, I implemented functions to compute:\n",
        "\n",
        "- **Cross Entropy:** Measures the average number of bits needed to encode a sentence.\n",
        "- **Perplexity:** A transformed version of cross entropy that provides a more intuitive measure of model performance.\n",
        "\n",
        "This evaluation is performed on a held-out set (a subset of the test sentences), providing a way to compare different models (bigram, trigram, and interpolated) and select the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bigram_prob(w1, w2, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed bigram probability:\n",
        "      P(w2|w1) = (C(w1, w2) + alpha) / (C(w1) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w1,), 0)\n",
        "    return (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "\n",
        "def compute_trigram_prob(w1, w2, w3, alpha):\n",
        "    \"\"\"\n",
        "    Computes smoothed trigram probability:\n",
        "      P(w3|w1,w2) = (C(w1, w2, w3) + alpha) / (C(w1, w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    c_trigram = ngram_counts.get(3, {}).get((w1, w2, w3), 0)\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w1, w2), 0)\n",
        "    return (c_trigram + alpha) / (c_bigram + alpha * V)\n",
        "\n",
        "def compute_interpolated_prob(w1, w2, w3, alpha, lam):\n",
        "    \"\"\"\n",
        "    Computes the interpolated probability:\n",
        "      P(w3|w1,w2) = lam * P_trigram(w3|w1,w2) + (1 - lam) * P_bigram(w3|w2)\n",
        "    where the bigram probability is computed as:\n",
        "      P(w3|w2) = (C(w2, w3) + alpha) / (C(w2) + alpha * |V|)\n",
        "    \"\"\"\n",
        "    # Trigram probability\n",
        "    p_tri = compute_trigram_prob(w1, w2, w3, alpha)\n",
        "    \n",
        "    # Bigram probability: note we use context w2 only\n",
        "    c_bigram = ngram_counts.get(2, {}).get((w2, w3), 0)\n",
        "    c_unigram = ngram_counts.get(1, {}).get((w2,), 0)\n",
        "    p_bi = (c_bigram + alpha) / (c_unigram + alpha * V)\n",
        "    \n",
        "    return lam * p_tri + (1 - lam) * p_bi\n",
        "\n",
        "\n",
        "def compute_cross_entropy_and_perplexity(sentences, model='interpolated', alpha=0.1, lam=0.5):\n",
        "    \"\"\"\n",
        "    Computes cross entropy and perplexity for a list of sentences.\n",
        "    \n",
        "    Parameters:\n",
        "      sentences: list of sentences (strings).\n",
        "      model: 'bigram', 'trigram', or 'interpolated'.\n",
        "      alpha: smoothing parameter.\n",
        "      lam: interpolation weight (only used if model=='interpolated').\n",
        "      \n",
        "    Returns:\n",
        "      cross_entropy, perplexity\n",
        "    \"\"\"\n",
        "    total_log_prob = 0.0\n",
        "    count = 0\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.lower().split()\n",
        "        if model == 'bigram':\n",
        "            for i in range(1, len(tokens)):\n",
        "                prob = compute_bigram_prob(tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'trigram':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_trigram_prob(tokens[i-2], tokens[i-1], tokens[i], alpha)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "        elif model == 'interpolated':\n",
        "            for i in range(2, len(tokens)):\n",
        "                prob = compute_interpolated_prob(tokens[i-2], tokens[i-1], tokens[i], alpha, lam)\n",
        "                total_log_prob += math.log2(prob)\n",
        "                count += 1\n",
        "    cross_entropy = - total_log_prob / count if count > 0 else float('inf')\n",
        "    perplexity = 2 ** cross_entropy\n",
        "    return cross_entropy, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning via Grid Search\n",
        "\n",
        "I performed a grid search over candidate values for the smoothing parameter (`alpha`) and the interpolation weight (`lam`):\n",
        "\n",
        "- For the **bigram** and **trigram** models, I tuned the smoothing parameter.\n",
        "- For the **interpolated** model, both `alpha` and `lam` were tuned.\n",
        "- The best parameters were chosen based on the minimum cross entropy achieved on the held-out validation set.\n",
        "\n",
        "This tuning ensures that the language model is well-calibrated and provides reliable probability estimates for the subsequent correction task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning hyperparameters for the bigram LM:\n",
            "Best bigram hyperparameters (alpha, ): (0.01,) with Cross Entropy: 10.778102599040484 \n",
            "\n",
            "Tuning hyperparameters for the trigram LM:\n",
            "Best trigram hyperparameters (alpha, ): (0.0001,) with Cross Entropy: 9.07821429240082 \n",
            "\n",
            "Tuning hyperparameters for the interpolated LM:\n",
            "Best interpolated hyperparameters (alpha, lambda): (0.0001, 0.75) with Cross Entropy: 8.709548759321516 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "held_out = train_sentences[:10000]\n",
        "\n",
        "# Define candidate values for alpha and lambda.\n",
        "alpha_values = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1.0]\n",
        "lam_values = [0.0001, 0.001, 0.01, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "best_bigram_params=None\n",
        "best_bigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the bigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='bigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_bigram_ce:\n",
        "        best_bigram_ce = ce\n",
        "        best_bigram_params = (alpha,)\n",
        "print(\"Best bigram hyperparameters (alpha, ):\", best_bigram_params, \"with Cross Entropy:\", best_bigram_ce, \"\\n\")\n",
        "\n",
        "best_trigram_params=None\n",
        "best_trigram_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the trigram LM:\")\n",
        "for alpha in alpha_values:\n",
        "    ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='trigram', alpha=alpha)\n",
        "    # print(f\"Alpha: {alpha}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "    if ce < best_trigram_ce:\n",
        "        best_trigram_ce = ce\n",
        "        best_trigram_params = (alpha,)\n",
        "print(\"Best trigram hyperparameters (alpha, ):\", best_trigram_params, \"with Cross Entropy:\", best_trigram_ce, \"\\n\")\n",
        "\n",
        "best_interpolated_params = None\n",
        "best_interpolated_ce = float('inf')\n",
        "print(\"Tuning hyperparameters for the interpolated LM:\")\n",
        "for alpha in alpha_values:\n",
        "    for lam in lam_values:\n",
        "        ce, ppl = compute_cross_entropy_and_perplexity(held_out, model='interpolated', alpha=alpha, lam=lam)\n",
        "        # print(f\"Alpha: {alpha}, Lambda: {lam}, Cross Entropy: {ce:.4f}, Perplexity: {ppl:.4f}\")\n",
        "        if ce < best_interpolated_ce:\n",
        "            best_interpolated_ce = ce\n",
        "            best_interpolated_params = (alpha, lam)\n",
        "\n",
        "print(\"Best interpolated hyperparameters (alpha, lambda):\", best_interpolated_params, \"with Cross Entropy:\", best_interpolated_ce, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The best model is interpolated**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context-Sensitive Correction using Beam Search\n",
        "\n",
        "The final component of the solution is a context-sensitive correction function (`my_correction`):\n",
        "\n",
        "- **Beam Search:** Instead of correcting each word independently, the beam search maintains a set of the best candidate sequences (with a defined beam width) as it processes the sentence word by word.\n",
        "- **Context-aware Scoring:** For each word, candidate corrections are scored based on the n-gram probability computed with the appropriate context. Early in the sentence (with little context), unigram or bigram probabilities are used; with more context, interpolated trigram probabilities are applied.\n",
        "- **Caching Corrections:** The `lru_cache` decorator is used to speed up candidate generation by caching results for repeated words.\n",
        "\n",
        "This approach enables the corrector to select a globally coherent sequence of words, thus effectively addressing context-sensitive ambiguities (e.g., distinguishing “doing sport” from “dying sport” based on the surrounding words).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_alpha = best_interpolated_params[0]\n",
        "best_lambda = best_interpolated_params[1]\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_corrections(word, vocabulary=my_vocab):\n",
        "    def filter_known(candidates, vocab):\n",
        "        return {w for w in candidates if w in vocab}\n",
        "    \n",
        "    candidates = filter_known([word], vocabulary)\n",
        "    if candidates:\n",
        "        return candidates\n",
        "    candidates = filter_known(edits1(word), vocabulary)\n",
        "    if candidates:\n",
        "        return candidates\n",
        "    candidates = filter_known(edits2(word), vocabulary)\n",
        "    if candidates:\n",
        "        return candidates\n",
        "    return {word}\n",
        "\n",
        "\n",
        "def my_correction(sentence, beam_width=3, n=2, alpha=best_alpha, lam=best_lambda):\n",
        "    \"\"\"\n",
        "    Corrects a sentence using an n-gram language model with tuned hyperparameters.\n",
        "    \n",
        "    Parameters:\n",
        "      - sentence: the input sentence to be corrected.\n",
        "      - beam_width: the beam width for search.\n",
        "      - n: the n-gram order to use (e.g., n=2 for bigrams, n=3 for trigrams).\n",
        "      - alpha: smoothing parameter (tuned).\n",
        "      - lam: interpolation weight for trigram and bigram probabilities (tuned).\n",
        "    \n",
        "    For each word, we use a context of up to (n-1) preceding words.\n",
        "    When there isn’t enough context (at the start), we back off to lower-order models.\n",
        "    \"\"\"\n",
        "    sentence_words = sentence.lower().split()\n",
        "    candidates_seq = [(0.0, [])]\n",
        "    \n",
        "    for i, word in enumerate(sentence_words):\n",
        "        new_candidates = []\n",
        "        candidate_list = list(get_corrections(word))\n",
        "        \n",
        "        for candidate in candidate_list:\n",
        "            for score, seq in candidates_seq:\n",
        "                context_length = min(n - 1, len(seq))\n",
        "                if context_length == 0:\n",
        "                    # Use unigram probability with smoothing using alpha.\n",
        "                    prob = (ngram_counts[1].get((candidate,), 0) + alpha) / (total_words + alpha * V)\n",
        "                elif context_length == 1:\n",
        "                    # Use bigram probability with smoothing.\n",
        "                    context = tuple(seq[-1:])\n",
        "                    prob = (ngram_counts.get(2, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(1, {}).get(context, 0) + alpha * V)\n",
        "                elif context_length == 2:\n",
        "                    # Use interpolated probability for trigram: \n",
        "                    # P(candidate|w1,w2) = lam * trigram + (1-lam) * bigram\n",
        "                    w1, w2 = seq[-2], seq[-1]\n",
        "                    trigram_prob = (ngram_counts.get(3, {}).get((w1, w2, candidate), 0) + alpha) / (\n",
        "                                   ngram_counts.get(2, {}).get((w1, w2), 0) + alpha * V)\n",
        "                    bigram_prob = (ngram_counts.get(2, {}).get((w2, candidate), 0) + alpha) / (\n",
        "                                  ngram_counts.get(1, {}).get((w2,), 0) + alpha * V)\n",
        "                    prob = lam * trigram_prob + (1 - lam) * bigram_prob\n",
        "                else:\n",
        "                    # For contexts longer than 2, fall back to the standard n-gram formula with smoothing.\n",
        "                    context = tuple(seq[-context_length:])\n",
        "                    prob = (ngram_counts.get(context_length + 1, {}).get(context + (candidate,), 0) + alpha) / (\n",
        "                           ngram_counts.get(context_length, {}).get(context, 0) + alpha * V)\n",
        "                \n",
        "                new_score = score + math.log(prob)\n",
        "                new_seq = seq + [candidate]\n",
        "                heapq.heappush(new_candidates, (new_score, new_seq))\n",
        "                if len(new_candidates) > beam_width:\n",
        "                    heapq.heappop(new_candidates)\n",
        "        \n",
        "        candidates_seq = new_candidates\n",
        "    \n",
        "    best_score, best_seq = max(candidates_seq, key=lambda x: x[0])\n",
        "    return ' '.join(best_seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid Search for Optimal Beam Width\n",
        "\n",
        "In this section, we perform a grid search to determine the best `beam_width` for our context-sensitive spelling correction algorithm. The `beam_width` parameter controls how many candidate correction sequences are maintained during the beam search. While a higher beam width might capture more promising candidate sequences and potentially yield higher accuracy, it also increases computational cost.\n",
        "\n",
        "For this grid search:\n",
        "- We evaluate a range of beam widths (from 1 to 10).\n",
        "- For each beam width, we add noise to sentences (using our `add_noise` function) to simulate spelling errors.\n",
        "- We run the `my_correction` function on the noisy sentences (with `n=3` for trigram context) and compare the output with the original sentence.\n",
        "- The overall accuracy is calculated as the proportion of words correctly recovered.\n",
        "- Finally, we plot the beam width values against their corresponding accuracies to visualize and select the best parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating synthetic train data:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating synthetic train data: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n"
          ]
        }
      ],
      "source": [
        "def add_noise(word, noise_prob=0.2):\n",
        "    if random.random() > noise_prob or len(word) < 1:\n",
        "        return word\n",
        "    edits_set = set()\n",
        "\n",
        "    for edit1_word in edits1(word):\n",
        "        for edit2_word in edits1(edit1_word):\n",
        "            edits_set.add(edit2_word)\n",
        "    \n",
        "    edits_set = edits_set.union(edits1(word))\n",
        "    edits = list(edits_set)\n",
        "        \n",
        "    return random.choice(edits) if edits else word\n",
        "\n",
        "test_data = []\n",
        "for sentence in tqdm(train_sentences[:100], desc=\"Generating synthetic train data\"):\n",
        "    original = words(sentence)\n",
        "    noisy = [add_noise(word, 0.7) for word in original]\n",
        "    test_data.append([original, noisy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting grid search for beam width...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=1: 100%|██████████| 100/100 [00:33<00:00,  3.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 1: Accuracy = 0.6916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=3: 100%|██████████| 100/100 [00:00<00:00, 2118.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 3: Accuracy = 0.7219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=5: 100%|██████████| 100/100 [00:00<00:00, 1410.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 5: Accuracy = 0.7302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=7: 100%|██████████| 100/100 [00:00<00:00, 1041.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 7: Accuracy = 0.7340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=9: 100%|██████████| 100/100 [00:00<00:00, 825.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 9: Accuracy = 0.7368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=11: 100%|██████████| 100/100 [00:00<00:00, 699.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 11: Accuracy = 0.7373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=13: 100%|██████████| 100/100 [00:00<00:00, 604.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 13: Accuracy = 0.7379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=15: 100%|██████████| 100/100 [00:00<00:00, 493.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 15: Accuracy = 0.7373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=17: 100%|██████████| 100/100 [00:00<00:00, 446.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 17: Accuracy = 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating beam_width=19: 100%|██████████| 100/100 [00:00<00:00, 414.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam width 19: Accuracy = 0.7379\n",
            "\n",
            "Best beam_width: 17 with Accuracy: 0.7384\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "beam_width_values = list(range(1, 20, 2))\n",
        "accuracy_results = []\n",
        "\n",
        "print(\"Starting grid search for beam width...\\n\")\n",
        "\n",
        "for bw in beam_width_values:\n",
        "    my_correct, total = 0, 0\n",
        "    for original, noisy in tqdm(test_data, desc=f\"Evaluating beam_width={bw}\"):\n",
        "        noisy_sentence = ' '.join(noisy)\n",
        "        my_result = my_correction(noisy_sentence, n=3, beam_width=bw).split()\n",
        "        \n",
        "        if len(my_result) != len(original):\n",
        "            continue\n",
        "        \n",
        "        for o, m in zip(original, my_result):\n",
        "            total += 1\n",
        "            if m == o:\n",
        "                my_correct += 1\n",
        "    \n",
        "    acc = my_correct / total if total > 0 else 0\n",
        "    accuracy_results.append(acc)\n",
        "    print(f\"Beam width {bw}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "best_idx = accuracy_results.index(max(accuracy_results))\n",
        "best_beam_width = beam_width_values[best_idx]\n",
        "print(f\"\\nBest beam_width: {best_beam_width} with Accuracy: {accuracy_results[best_idx]:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(beam_width_values, accuracy_results, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Beam Width\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Beam Width vs. Spelling Correction Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on a Test Set\n",
        "\n",
        "In this section, we evaluate our spelling correction system using a real-world dataset – the Holbrook corpus. The Holbrook dataset consists of passages taken from the book *English for the Rejected* by David Holbrook. These passages are unique because they include genuine writing errors from secondary-school children. The original misspellings are tagged, which allows us to compare the corrected output against the intended words.\n",
        "\n",
        "We will compare the performance of two systems:\n",
        "- **Norvig's Corrector:** Our baseline implementation based on Norvig's algorithm.\n",
        "- **NGram-based Corrector:** Our enhanced context-sensitive corrector that uses an n-gram language model with beam search.\n",
        "\n",
        "The evaluation metrics include accuracy, precision, recall, and F1 score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parsing the Holbrook Dataset\n",
        "\n",
        "The function `parse_holbrook_line` processes each line from the Holbrook dataset. It extracts:\n",
        "- The **raw sentence** (with errors),\n",
        "- The **correct sentence** (with target words), and\n",
        "- The **error positions** (indexes where errors occurred).\n",
        "\n",
        "This parsing is essential for aligning the outputs of our correctors with the reference text, ensuring we know exactly where corrections should have been made.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    - List of error positions and targets\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    # Split line into text and error segments\n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "        \n",
        "        if len(error.split()) > 1 or len(target.split()) > 1:\n",
        "            raw_parts.append(target)\n",
        "            continue\n",
        "        else:\n",
        "            raw_parts.append(error)\n",
        "        \n",
        "\n",
        "        if error == target:\n",
        "            continue\n",
        "\n",
        "        error_start = len(' '.join(raw_parts).split())\n",
        "        errors.append(error_start-1)\n",
        "    \n",
        "    \n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence, errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Metrics on the Holbrook Dataset\n",
        "\n",
        "The `evaluate_holbrook` function evaluates a given spelling corrector on the parsed Holbrook data. For each sentence, it:\n",
        "- Applies the correction function (either Norvig's or our NGram-based method).\n",
        "- Compares the corrected sentence to the correct sentence (the ground truth).\n",
        "- Counts the total number of errors, the number of corrections proposed by the system, and the number of correct corrections.\n",
        "\n",
        "From these counts, we calculate:\n",
        "- **Accuracy:** The percentage of errors that were correctly corrected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_holbrook(test_lines, corrector_func, beam_width=None, n=None):\n",
        "    \"\"\"\n",
        "    Evaluate spelling corrector on Holbrook dataset\n",
        "    Returns:\n",
        "    - Accuracy: % of errors corrected properly\n",
        "    \"\"\"\n",
        "    stats = defaultdict(int)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence, errors = parse_holbrook_line(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        stats[\"total_errors\"] += len(errors)\n",
        "        \n",
        "        for i, (raw, corrected, correct) in enumerate(zip(raw_sentence.split(), corrected_sentence.split(), correct_sentence.split())):\n",
        "            if raw != corrected:\n",
        "                stats['total_proposed'] += 1\n",
        "\n",
        "            if i in errors and corrected == correct:\n",
        "                stats['correct'] += 1\n",
        "    \n",
        "    accuracy = stats['correct'] / stats['total_errors'] if stats['total_errors'] else 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'total_errors': stats['total_errors'],\n",
        "        'correct_corrections': stats['correct'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading and Inspecting the Holbrook Data\n",
        "\n",
        "We load the Holbrook dataset from the file `holbrook-tagged.dat.txt` and split the text into sentences using regular expressions. A sample sentence is printed and parsed to illustrate how the errors are tagged and how the parsing function works. This step ensures that our evaluation functions receive correctly formatted data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186633\n",
            "my <err targ=sister> siter </err> <err targ=goes> go </err> to tonbury.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('my siter go to tonbury.', 'my sister goes to tonbury.', [1, 2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('holbrook-tagged.dat.txt') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print(len(text))\n",
        "\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "\n",
        "sentence_index = 3\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Norvig's Correction on the Holbrook Dataset\n",
        "\n",
        "The first evaluation is performed using Norvig's correction function. After processing the entire dataset with `evaluate_holbrook`, we print out the computed accuracy, precision, recall, and F1 score. This serves as the baseline performance for our spelling correction system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:27<00:00, 37.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 17.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "norvig_metrics = evaluate_holbrook(sentences, norvigs_correction)\n",
        "\n",
        "print(f\"Accuracy: {norvig_metrics['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the NGram-based Correction on the Holbrook Dataset\n",
        "\n",
        "Next, we evaluate our enhanced correction system (which uses beam search with a trigram language model) on the same dataset. In this case, we call `evaluate_holbrook` with the additional parameters `n=3` (for trigram context) and an appropriate `beam_width` (e.g., 5). The results are printed and later compared with those from Norvig's corrector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:23<00:00, 44.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 23.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ngram_metrics = evaluate_holbrook(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "\n",
        "print(f\"Accuracy: {ngram_metrics['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing the Results\n",
        "\n",
        "After obtaining the metrics for both approaches, we calculate and print the differences between the algorithms' results. This comparison includes:\n",
        "- The improvement in accuracy,\n",
        "\n",
        "This side-by-side comparison helps illustrate the impact of incorporating context through the n-gram model and beam search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparative Improvement:\n",
            "Accuracy Improvement: 5.73%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nComparative Improvement:\")\n",
        "accuracy_improvement = ngram_metrics['accuracy'] - norvig_metrics['accuracy']\n",
        "\n",
        "print(f\"Accuracy Improvement: {accuracy_improvement:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Word Error Rate (WER)\n",
        "\n",
        "WER is a widely used metric in tasks like speech recognition and spelling correction. It is defined as:\n",
        "\n",
        "### $ WER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of substitutions\n",
        "* $ i $ : the number of insertions\n",
        "* $ D $ : the number of deletions\n",
        "* N: the total number of words in the reference\n",
        "\n",
        "To compute WER, we first define a helper function `parse_holbrook_line_without_indexes` which returns the raw and correct sentences without the error index information. The `evaluate_wer` function then uses the `nltk.edit_distance` function (i.e., Levenshtein distance) on word sequences to determine the number of word-level errors. We run this evaluation for both Norvig's corrector and our NGram-based corrector to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_holbrook_line_without_indexes(line):\n",
        "    \"\"\"\n",
        "    Parse a line from Holbrook dataset, extracting:\n",
        "    - Original sentence with errors\n",
        "    - Correct sentence with targets\n",
        "    \"\"\"\n",
        "    raw_parts = []\n",
        "    correct_parts = []\n",
        "    \n",
        "    pos = 0\n",
        "    for match in re.finditer(r'<err targ=(.*?)>(.*?)</err>', line):\n",
        "        target = match.group(1).strip('\"')\n",
        "        error = match.group(2)\n",
        "\n",
        "        start, end = match.start(), match.end()\n",
        "        \n",
        "        raw_parts.append(line[pos:start])\n",
        "        correct_parts.append(line[pos:start])\n",
        "        \n",
        "        pos = end\n",
        "\n",
        "        correct_parts.append(target)\n",
        "\n",
        "        raw_parts.append(error)\n",
        "\n",
        "    raw_parts.append(line[pos:])\n",
        "    correct_parts.append(line[pos:])\n",
        "    \n",
        "    raw_sentence = ' '.join(''.join(raw_parts).split())\n",
        "    correct_sentence = ' '.join(''.join(correct_parts).split())\n",
        "    \n",
        "    return raw_sentence, correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my <err targ=sister> siter </err> <err targ=goes> go </err> to tonbury.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('my siter go to tonbury.', 'my sister goes to tonbury.')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_index = 3\n",
        "print(sentences[sentence_index])\n",
        "\n",
        "parse_holbrook_line_without_indexes(sentences[sentence_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "Levenshtein =  nltk.edit_distance\n",
        "\n",
        "def evaluate_wer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "\n",
        "    total_wer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        reference_words = correct_sentence.split()\n",
        "        corrected_words = corrected_sentence.split()\n",
        "\n",
        "        S = Levenshtein(reference_words, corrected_words)\n",
        "        I = max(0, len(corrected_words) - len(reference_words))\n",
        "        D = max(0, len(reference_words) - len(corrected_words))\n",
        "\n",
        "        N = max(len(reference_words), len(corrected_words))\n",
        "\n",
        "        wer = (S + I + D) / N\n",
        "        total_wer += wer\n",
        "\n",
        "    return total_wer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/1057 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:29<00:00, 36.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's WER: 0.22662652083405424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "wer_norvig = evaluate_wer(sentences, norvigs_correction)\n",
        "print(\"Norvig's WER:\", wer_norvig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:03<00:00, 287.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGram-based WER: 0.2835148408766171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "wer_ngram = evaluate_wer(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "print(\"NGram-based WER:\", wer_ngram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER Difference (NGram-based WER - Norvig's WER): 0.05688832004256286\n"
          ]
        }
      ],
      "source": [
        "improvement_wer = wer_ngram - wer_norvig\n",
        "print(\"WER Difference (NGram-based WER - Norvig's WER):\", improvement_wer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**WER increased due to NGram algorithm found more errors than Norvigs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character Error Rate (CER)\n",
        "\n",
        "CER is similar to WER but is calculated at the character level. This metric is particularly useful for capturing fine-grained errors that might be overlooked by word-level evaluation. The formula for CER is:\n",
        "\n",
        "\n",
        "### $ CER = \\frac{S + I + D}{N} $\n",
        "\n",
        "* $ S $ : the number of character substitutions\n",
        "* $ i $ : the number of character insertions\n",
        "* $ D $ : the number of character deletions\n",
        "* N: the total number of characterσ in the reference\n",
        "\n",
        "where the variables represent the number of character substitutions, insertions, deletions, and the total number of characters in the reference, respectively.\n",
        "\n",
        "The `evaluate_cer` function computes CER by comparing the corrected output and the reference at the character level using the Levenshtein distance. We evaluate both Norvig's and our NGram-based corrector on the Holbrook dataset and compare the resulting CER scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_cer(test_lines, corrector_func, beam_width=None, n=None):\n",
        "    total_cer = 0\n",
        "    num_sentences = len(test_lines)\n",
        "    \n",
        "    for line in tqdm(test_lines, desc='Evaluating'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "            \n",
        "        raw_sentence, correct_sentence = parse_holbrook_line_without_indexes(line)\n",
        "        \n",
        "        if (beam_width is None) or (n is None):\n",
        "            corrected_sentence = corrector_func(raw_sentence)\n",
        "        else:    \n",
        "            corrected_sentence = corrector_func(raw_sentence, beam_width=beam_width, n=n)\n",
        "        \n",
        "        S = Levenshtein(correct_sentence, corrected_sentence)\n",
        "        I = max(0, len(corrected_sentence) - len(correct_sentence))\n",
        "        D = max(0, len(correct_sentence) - len(corrected_sentence))\n",
        "\n",
        "        N = max(len(correct_sentence), len(corrected_sentence))\n",
        "\n",
        "        cer = (S + I + D) / N\n",
        "        total_cer += cer\n",
        "\n",
        "    return total_cer / num_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:43<00:00, 24.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's WER: 0.10763259303603535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cer_norvig = evaluate_cer(sentences, norvigs_correction)\n",
        "print(\"Norvig's WER:\", cer_norvig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1057/1057 [00:17<00:00, 61.05it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGram-based WER: 0.11844231247494862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cer_ngram = evaluate_cer(sentences, my_correction, n=3, beam_width=best_beam_width)\n",
        "print(\"NGram-based WER:\", cer_ngram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CER Difference (NGram-based WER - Norvig's WER): 0.010809719438913262\n"
          ]
        }
      ],
      "source": [
        "improvement_cer = cer_ngram - cer_norvig\n",
        "print(\"CER Difference (NGram-based WER - Norvig's WER):\", improvement_cer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CER increased due to NGram algorithm found more errors than Norvigs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reporting Differences Between Algorithms\n",
        "\n",
        "Finally, we calculate the differences between the performance metrics (accuracy, WER, CER, etc.) of Norvig's corrector and our NGram-based corrector. These differences are printed out, providing a clear picture of how much our improvements have enhanced the overall spelling correction performance. This comparative analysis is crucial for validating the effectiveness of the context-sensitive approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparative Improvement:\n",
            "Accuracy Improvement: 5.73%\n",
            "WER Difference (NGram-based WER - Norvig's WER): 0.05688832004256286\n",
            "CER Difference (NGram-based CER - Norvig's CER): 0.010809719438913262\n",
            "\n",
            "Comparison Table:\n",
            "Metric     Norvig's Corrector        NGram-based Corrector     Difference (NGram - Norvig)\n",
            "Accuracy   17.68%                    23.40%                    5.73%                    \n",
            "WER        0.2266                    0.2835                    0.0569                   \n",
            "CER        0.1076                    0.1184                    0.0108                   \n"
          ]
        }
      ],
      "source": [
        "accuracy_improvement = ngram_metrics['accuracy'] - norvig_metrics['accuracy']\n",
        "improvement_wer = wer_ngram - wer_norvig\n",
        "improvement_cer = cer_ngram - cer_norvig\n",
        "\n",
        "print(\"\\nComparative Improvement:\")\n",
        "print(f\"Accuracy Improvement: {accuracy_improvement:.2%}\")\n",
        "print(\"WER Difference (NGram-based WER - Norvig's WER):\", improvement_wer)\n",
        "print(\"CER Difference (NGram-based CER - Norvig's CER):\", improvement_cer)\n",
        "\n",
        "table_data = [\n",
        "    [\"Metric\", \"Norvig's Corrector\", \"NGram-based Corrector\", \"Difference (NGram - Norvig)\"],\n",
        "    [\"Accuracy\", f\"{norvig_metrics['accuracy']:.2%}\", f\"{ngram_metrics['accuracy']:.2%}\", f\"{accuracy_improvement:.2%}\"],\n",
        "    [\"WER\", f\"{wer_norvig:.4f}\", f\"{wer_ngram:.4f}\", f\"{improvement_wer:.4f}\"],\n",
        "    [\"CER\", f\"{cer_norvig:.4f}\", f\"{cer_ngram:.4f}\", f\"{improvement_cer:.4f}\"]\n",
        "]\n",
        "\n",
        "print(\"\\nComparison Table:\")\n",
        "for row in table_data:\n",
        "    print(\"{:<10} {:<25} {:<25} {:<25}\".format(*row))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparative Analysis\n",
        "\n",
        "The table above summarizes the performance of the two spelling correction algorithms on the Holbrook dataset. Our NGram-based corrector shows significant improvement over Norvig's corrector, with an accuracy improvement of **5.73%**. In addition, the NGram-based method demonstrates higher Word Error Rate (WER) and Character Error Rate (CER) compared to the baseline, because it found more errors\n",
        "\n",
        "This improvement indicates that incorporating context through n-gram language modeling and beam search enables our algorithm to better capture the intended word sequence, leading to more accurate corrections. These results validate the effectiveness of our context-sensitive approach over the traditional Norvig's method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
